# (2023) ì œ 2íšŒ íœ´ë¨¼ì´í•´ ì¸ê³µì§€ëŠ¥ ë…¼ë¬¸ê²½ì§„ëŒ€íšŒ
> ë³¸ ëŒ€íšŒëŠ” í•œêµ­ì „ìí†µì‹ ì—°êµ¬ì›(ETRI)ì´ ì£¼ìµœí•˜ê³  ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€ì™€ êµ­ê°€ê³¼í•™ê¸°ìˆ ì—°êµ¬íšŒ(NST)ê°€ í›„ì›í•©ë‹ˆë‹¤
---
## Multi-model Emotion Recognition Model based on Temporal Graph Learning
## Abstract
> ìµœê·¼ ë”¥ëŸ¬ë‹ ê¸°ìˆ ì´ ë°œì „í•˜ë©´ì„œ, ì¸ê°„-ì»´í“¨í„° ìƒí˜¸ì‘ìš©(HCI) ë¶„ì•¼ì˜ ì—°êµ¬ê°€ í™œë°œíˆ ì§„í–‰ë˜ê³  ìˆë‹¤. ê°ì • ì¸ì‹ ë¶„ì•¼ëŠ” HI ì—°êµ¬ ë¶„ì•¼ì˜ ì£¼ìš” ê³¼ì œ ì¤‘ í•˜ë‚˜ì´ë‹¤. ê°ì • ì¸ì‹ì„ í†µí•´ ì‚¬ìš©ìëŠ” ë”ìš± í˜„ì‹¤ê° ìˆëŠ” ì‚¬ìš©ì ê²½í—˜ì„ ì–»ì„ ìˆ˜ ìˆë‹¤. ë›°ì–´ë‚œ ì‚¬ìš©ì ê²½í—˜ì„ ì œê³µí•˜ê¸° ìœ„í•´ì„œëŠ” ë†’ì€ ì •í™•ë„ê°€ ìš”êµ¬ëœë‹¤. ìµœê·¼ ë”¥ëŸ¬ë‹ì—ì„œëŠ” 'ë©€í‹° ëª¨ë‹¬' ê¸°ìˆ ì„ í†µí•˜ì—¬, ì—¬ëŸ¬ ëª¨ë‹¬ë¦¬í‹°ì— ìˆëŠ” ë°ì´í„°ë¥¼ ìœµí•©í•˜ì—¬ ë†’ì€ ì •í™•ë„ë¥¼ ì–»ê³ ì í•˜ëŠ” ì‹œë„ë¥¼ í•˜ì˜€ë‹¤. ì´ì— ë³¸ ì—°êµ¬ì—ì„œëŠ”, ì¸ì½”ë”ë¥¼ í†µí•œ ì¢‹ì€ ì ì¬ ê³µê°„ì„ êµ¬í•˜ê³ , ì´ë¥¼ GNN ëª¨ë¸ ê¸°ë°˜ ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•˜ì—¬ ê°ì • ì¸ì‹ ì‘ì—…ì„ ì§„í–‰í–ˆë‹¤. ì´ ì‘ì—…ì„ ì§„í–‰í•˜ê¸° ìœ„í•˜ì—¬ ë°œí™” ìƒí™©ì˜ ì˜¤ë””ì˜¤ ë°ì´í„°, EDA ë°ì´í„° ê·¸ë¦¬ê³  ì˜¨ë„ ë°ì´í„°ë¥¼ ìœµí•©í•˜ì—¬ ì‚¬ìš©í–ˆë‹¤. ë©€í‹° ëª¨ë‹¬ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬, ë‹¤ì–‘í•œ ë‹¨ì„œë¥¼ ëª¨ë¸ì˜ í•™ìŠµì— ì´ìš©í•  ìˆ˜ ìˆê²Œë˜ì—ˆë‹¤.

## 1. ì†Œê°œ
### 1.1 ëŒ€íšŒ ì†Œê°œ
#### ë©€í‹°ëª¨ë‹¬ ê°ì • ë°ì´í„°ì…‹ í™œìš© ê°ì • ì¸ì‹ ê¸°ìˆ  ë¶„ì•¼
- ì¸ê°„ê³¼ êµê°í•  ìˆ˜ ìˆëŠ” ì¸ê³µì§€ëŠ¥ êµ¬í˜„ì„ ìœ„í•´ì„œëŠ” ì¸ê°„ì˜ í–‰ë™ê³¼ ê°ì •ì„ ì´í•´í•˜ëŠ” ê¸°ìˆ ì´ í•„ìš”í•©ë‹ˆë‹¤.
- ì‚¬ëŒì˜ í–‰ë™ê³¼ ê°ì •ì„ ì´í•´í•˜ëŠ” ê¸°ìˆ  ì—°êµ¬ë¥¼ ê°€ëŠ¥í† ë¡ í•˜ê¸° ìœ„í•´ êµ¬ì¶•í•œ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ íœ´ë¨¼ì´í•´ ì¸ê³µì§€ëŠ¥ ê¸°ìˆ  ì—°êµ¬ë¥¼ í™•ì‚°ì‹œí‚¤ê³  ì°½ì˜ì ì¸ ì—¬êµ¬ë¥¼ ë°œêµ´í•˜ê³ ì ETRIì—ì„œ ëŒ€íšŒë¥¼ ê°œìµœí–ˆìŠµë‹ˆë‹¤.

- TaskëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
    - ìš°ë¦¬ëŠ” ë³¸ ì—°êµ¬ì—ì„œ ì¼ë°˜ì¸ ëŒ€ìƒ ììœ ë°œí™”:[KEMDy20](https://nanum.etri.re.kr/share/kjnoh/KEMDy20?lang=ko_KR) ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ ê°ì •ì˜ ë ˆì´ë¸”(ê¸°ì¨, ë†€ëŒ, ë¶„ë…¸, ì¤‘ë¦½, í˜ì˜¤, ê³µí¬, ìŠ¬í””)ì— ëŒ€í•œ ë¶„ë¥˜ ì •í™•ë„(F1)ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.
    - ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ë¥¼ í˜¼í•©í•©ë‹ˆë‹¤. ë°œí™”ìŒì„±ê³¼ EDA ë°ì´í„°, ì˜¨ë„ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë©€í‹°ëª¨ë‹¬ ë°ì´í„° ê°ì •ì¸ì‹ ëª¨ë¸ì„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤.
    - ì„ë² ë”© ë²¡í„°ë¡œ graphë¥¼ ìƒì„±í•˜ì—¬ GNN ëª¨ë¸ì— í†µê³¼ì‹œì¼œ ê°ì •ë¶„ë¥˜ ì˜ˆì¸¡ì„ í•©ë‹ˆë‹¤.


### 1.2 Methodolgy

#### Model Architecture
![model_architecture](./images/gnn.png)
ì°¸ì¡°í•œ [GNN model architecture](https://github.com/SRA2/SPELL) ì…ë‹ˆë‹¤. ì—°ê²°ì„ í•  ë•Œ ê´€ê³„ ì •ë³´ë¥¼ ì¶©ë¶„íˆ ë¶€ì—¬í•˜ê¸° ìœ„í•´ forward, backward, undirect conntectionì„ ì¶”ê°€í•˜ê³  3ê°œì˜ Layerë¥¼ í†µê³¼ í›„ í•©ì¹œ í›„ softmaxë¥¼ í†µí•´ 7ê°œì˜ ê°ì • í™•ë¥ ë¶„í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

#### Data Embedding
![embedding](./images/embedding.png)
ìŒì„±, EDA, ì˜¨ë„ ë°ì´í„°ë¥¼ ê°ê°ì˜ ì¸ì½”ë”ë¥¼ í†µí•´ ëª¨ë‹¬ ë³„ ì„ë² ë”© ë²¡í„°ë¥¼ êµ¬í•˜ê³  ë‹¤ì¸µ ì‹ ê²½ë§ì„ í†µí•˜ì—¬ ê°ê°ì˜ ì˜ˆì¸¡ê°’ì„ êµ¬í•©ë‹ˆë‹¤. ë˜í•œ, ì´ë ‡ê²Œ ì–»ì–´ì§„ ì„ë² ë”© ë²¡í„°ë¥¼ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ í•©í•˜ê³ , ë‹¤ì¸µ ì‹ ê²½ë§ì„ í†µê³¼í•˜ì—¬ ìƒˆë¡œìš´ ì˜ˆì¸¡ê°’ì„ êµ¬í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ ì–»ì–´ì§„ ì˜ˆì¸¡ê°’ ë„¤ ê°œë¥¼ ëª¨ë‘ í•©í•˜ì—¬, ì†ì‹¤ê°’ì„ êµ¬í•©ë‹ˆë‹¤.
###


### 1.3 ì½”ë“œ ì„¤ëª…

```data_loader.py``` : data loading for generating graph and training, validation

```generate_graph.py``` : ì„ë² ë”© ë²¡í„°ë¡œ graph ìƒì„±

```models_gnn.py``` : ìš°ë¦¬ì˜ ëª¨ë¸

```train_val.py``` : í•™ìŠµê³¼ ê²€ì¦ì„ ì§„í–‰

### 1.4 ë°ì´í„° ì „ì²˜ë¦¬
- ìš°ë¦¬ê°€ ìˆ˜í–‰í•œ ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì„ ì œì‹œí•©ë‹ˆë‹¤.
- KEMDy ë°ì´í„°ì…‹ì—ì„œ ê°ì • ë ˆì´ë¸”ì´ ìˆëŠ” ì‹œê°„ì˜ ë°ì´í„°ë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤. ì´í›„, ê°ì •ì„ ì˜ˆì¸¡í•˜ë ¤ëŠ” íŠ¹ì • ì‹œì ì´ ì •í•´ì§€ë©´ íŠ¹ì • ì‹œì ì—ì„œë¶€í„° ì‚¬ìš©ìê°€ ì„¤ì •í•œ ê¸¸ì´ë§Œí¼ì„ ì• ë’¤ì—ì„œ ìë¦…ë‹ˆë‹¤. ì´í›„ ê° ì‹œê°„ì— ë§ëŠ” ì˜¤ë””ì˜¤ ë°ì´í„°, EDA ë°ì´í„°, ì˜¨ë„ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
#### ìŒì„± ë°ì´í„°
![mel-spectrogram](./images/mel-spectrogram.png)
</br>
ìŒì„± ë°ì´í„°ì˜ ê²½ìš° mel-spectrogramì˜ ì´ë¯¸ì§€ë¡œ ê°€ì ¸ì™€ì„œ 2D ê¸°ë°˜ì˜ ResNet18 ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ë²¡í„°ë¥¼ êµ¬í•©ë‹ˆë‹¤.
#### EDA, ì˜¨ë„ ë°ì´í„°
EDAì™€ ì˜¨ë„ ë°ì´í„°ëŠ” 1D ê¸°ë°˜ì˜ ResNet18 ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ë²¡í„°ë¥¼ êµ¬í•©ë‹ˆë‹¤.
</br>

### 1.5 GNN
- ì¸ì½”ë” í•™ìŠµì„ í†µí•´ ì–»ì–´ë‚¸ íŠ¹ì§•ë“¤ì„ ì´ìš©í•˜ì—¬ ê·¸ë˜í”„ ë…¸ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì—°ê²°ì„ í•  ë•Œ ê´€ê³„ ì •ë³´ë¥¼ ì¶©ë¶„íˆ ë¶€ì—¬í•˜ê¸° ìœ„í•´ forward, backward, undirect conntectionì„ ì¶”ê°€í•©ë‹ˆë‹¤. í•™ìŠµí•œ GNNì˜ ë…¸ë“œë¥¼ ì´ìš©í•˜ì—¬ multi-class ë¶„ë¥˜ë¥¼ í†µí•´ í™”ìì˜ ê°ì •ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

## 2. How To Use?
- ì´ ì½”ë“œë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤
- ìˆœì„œì™€ ì§€ì‹œë¥¼ __ê·¸ëŒ€ë¡œ__ ë”°ë¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”

### 2.1 í™˜ê²½ì„¤ì •
0. ì—¬ëŸ¬ë¶„ì˜ PCë‚˜ ì„œë²„ì— GPUê°€ ìˆê³  cuda settingì´ ë˜ì–´ìˆì–´ì•¼í•©ë‹ˆë‹¤.
1. ì—¬ëŸ¬ë¶„ì˜ í™˜ê²½ì— ì´ repoë¥¼ cloneí•©ë‹ˆë‹¤ : ```git clone <this_repo>```
2. requirements librariesë¥¼ í™•ì¸í•©ë‹ˆë‹¤ : ```pip install -r requirements.txt```

### 2.2 ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
1. [KEMDy20](https://nanum.etri.re.kr/share/kjnoh/KEMDy20?lang=ko_KR) datasetì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ```multimodal-emotion/encoder/KEMDy20``` í´ë”ì— ë„£ìœ¼ì„¸ìš”. ë‹¤ìš´ë¡œë“œ ê¶Œí•œì„ ì‹ ì²­í•´ì•¼í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
2. [Google_Drive](https://drive.google.com/drive/folders/1gt9GnIN2CQ6RiYIkvErdGKsce2GJ1ECL)ì—ì„œ ```features.tar```ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì••ì¶•ì„ í’€ë©´ ```multimodal-emotion/features```ì— ê·¸ë˜í”„ë¥¼ ë§Œë“¤ê¸° ìœ„í•œ ì„ë² ë”© ë²¡í„° íŒŒì¼ì´ ìƒê¹ë‹ˆë‹¤.
3. [Google_Drive](https://drive.google.com/drive/folders/1gt9GnIN2CQ6RiYIkvErdGKsce2GJ1ECL)ì—ì„œ ```graphs.tar```ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì••ì¶•ì„ í’€ë©´ ```multimodal-emotion/graphs```ì— í•™ìŠµì„ ìœ„í•œ graph íŒŒì¼ë“¤ì´ ìƒê¹ë‹ˆë‹¤. 

- ìµœì¢…ì ìœ¼ë¡œ structureê°€ ì´ë ‡ê²Œ ë˜ì–´ìˆë‹¤ë©´ ëª¨ë“  ì¤€ë¹„ê°€ ëë‚¬ìŠµë‹ˆë‹¤!
```
<multimodal emotion>
                    â”” <encoder>
                    â”œâ”€â”€ core
                    â”‚   â”œâ”€â”€ clip_utils.py
                    â”‚   â”œâ”€â”€ config.py
                    â”‚   â”œâ”€â”€ custom_transforms.py
                    â”‚   â”œâ”€â”€ dataset.py
                    â”‚   â”œâ”€â”€ dataset _save.py
                    â”‚   â”œâ”€â”€ io.py
                    â”‚   â”œâ”€â”€ models.py
                    â”‚   â”œâ”€â”€ optimization.py
                    â”‚   â”œâ”€â”€ __pycache__
                    â”‚   â””â”€â”€ util.py
                    â”œâ”€â”€ data
                    â”‚   â”œâ”€â”€ C
                    â”‚   â”œâ”€â”€ extract_audio_tracks.py
                    â”‚   â”œâ”€â”€ extract_face_crops_time.py
                    â”‚   â”œâ”€â”€ get_utility_files.sh
                    â”‚   â””â”€â”€ slice_audio_tracks.py
                    â”œâ”€â”€ df_generator.ipynb
                    â”œâ”€â”€ feats_generator.ipynb
                    â”œâ”€â”€ KEMDy20 -> ../KEMDy20 **(link)**
                    â”œâ”€â”€ KEMDy20results.csv
                    â”œâ”€â”€ README.md
                    â”œâ”€â”€ scripts
                    â”‚   â”œâ”€â”€ dev_env.sh
                    â”‚   â””â”€â”€ downloads.sh
                    â”œâ”€â”€ STE_forward.py
                    â”œâ”€â”€ STE_TRAIN
                    â”‚   â”œâ”€â”€ ste_encoder
                    â”‚   â”œâ”€â”€ ste_encoder_cfg.json
                    â”‚   â””â”€â”€ ste_encoder_logs.csv
                    â””â”€â”€ STE_train.py
                    â”” <features>
                    â”œâ”€â”€ features_train
                    â””â”€â”€ features_val
                    â”” <graphs>
                    â”œâ”€â”€ resnet18-tsm-aug_2000_7.2_cin_fsimy # generate_graph.py ê¸°ë³¸ argumentí–ˆì„ ë•Œ ìƒê¸°ëŠ” í´ë”ëª…
                    â”‚   â”œâ”€â”€ train
                    â”‚   â”‚   â”œâ”€â”€ processed
                    â”‚   â””â”€â”€  val
                    â”‚   â”‚   â”œâ”€â”€ processed
                    â”œ train_val.py
                    â”œ models_gnn.py
                    â”œ data_loader.py
                    â”œ generate_graph.py
                    â”œ LICENSE
                    â”œ requirements.txt
                    â”” README.md                           
```

### 2.3 encoder ì‚¬ìš©ë²•
0. encoder ë””ë ‰í† ë¦¬ë¡œ ì´ë™í•´ì£¼ì„¸ìš”.

<code> cd ./encoder</code>
1. encoder ë‚´ë¶€ì—, KEMDy20 í´ë”ë¥¼ ë§í¬ì‹œì¼œì£¼ì„¸ìš”.

<code> ln -s .../KEMDy20 .../encoder/KEMDy20 </code>

2. encoder í•™ìŠµì— í•„ìš”í•œ, ê°ì¢… pkl, csv íŒŒì¼ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
> df_generator.ipynbì˜ cellì„ ëª¨ë‘ ì‹¤í–‰ì‹œì¼œì£¼ì„¸ìš”.

3. encoderë¥¼ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.
 ëª¨ë¸ ê´€ë ¨ config ì„¤ì •ì€ .../encoder/core/config.pyì—ì„œ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
     ê¸°ë³¸ì ìœ¼ë¡œ, encoderì˜ ê°€ì¤‘ì¹˜ëŠ” ./STE_TRAIN/ste_encoder/{}.pthì— ì €ì¥ë©ë‹ˆë‹¤.

<code> python STE_train <clip_lenght> <device> </code>
<code> ex) python STE_train 11 0 </code>

4. encoderë¡œ embeddig faetureë¥¼ ë½‘ì•„ì£¼ì„¸ìš”.
ğŸª„ ëª¨ë¸ ê´€ë ¨ config ì„¤ì •ì€ .../encoder/core/config.pyì—ì„œ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
     ë¶ˆëŸ¬ì˜¤ëŠ” ê°€ì¤‘ì¹˜ë¥¼ ë³€ê²½í•˜ê¸° ìœ„í•´ì„œ, config.py ë‚´ë¶€ì˜, STE_inputs['model_weights']ë¥¼ ë³€ê²½í•´ì£¼ì„¸ìš”.
     
<code> python STE_forward <clip_lenght> <device> </code>
<code> ex) python STE_forward 11 0 </code>

5. ìƒì„±ëœ embedding featureë¥¼ pkl íŒŒì¼ë¡œ, ì„¸ì…˜ë³„ë¡œ ë‚˜ëˆ ì„œ ì €ì¥í•´ì£¼ì„¸ìš”.
ğŸª„ ë¶ˆëŸ¬ì˜¤ëŠ” ê°€ì¤‘ì¹˜ë¥¼ ë³€ê²½í•˜ê¸° ìœ„í•´ì„œ, 2ë²ˆì§¸ cell ì˜ model_weights = './STE_TRAIN/ste_encoder/{}.pth'ë¥¼ ë³€ê²½í•´ì£¼ì„¸ìš”.

> feats_generator.ipynbì˜ cellì„ ëª¨ë‘ ì‹¤í–‰ì‹œì¼œì£¼ì„¸ìš”.
ìµœì¢…ì ìœ¼ë¡œ .../KEMDy20/Sessionì— sessionë³„ë¡œ ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ embedding featuresê°€ ì €ì¥ë©ë‹ˆë‹¤.


### 2.4 GNN í•™ìŠµ
#### ê·¸ë˜í”„ ìƒì„±
```
python generate_graph.py
```
#### Speaker ê°ì • í•™ìŠµ baseline
```
python train_val.py
```



## 3. ì„±ëŠ¥
### 3.1 F1 score

| Accuracy | F1 | Parameters |
| --- | --- | --- |
| 90.47 | 90.48 | 139,365 |



## Contact
- Junseok Yoon : phobyjun@khu.ac.kr
- Hong-Ju Jeong : sub06038@khu.ac.kr
- Inhun Choi : inhun321@khu.ac.kr
- Hyunjun Choi : kikitank1@khu.ac.kr
- Joonshik Hong : jshong0907@gmail.com

## Reference
[1] Patrick, M, et al. "Space-time crop & attend: Improving cross-modal video representation learning." arXiv preprint arXiv:2103.10211 (2021).
</br>
[2] He, Kaiming, et al. "Deep residual learning for image recognition." Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.
</br>
[3] Min, Kyle, et al. "Learning long-term spatial-temporal graphs for active speaker detection." Computer Visionâ€“ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23â€“27, 2022, Proceedings, Part XXXV. Cham: Springer Nature Switzerland, 2022.
</br>
[4] Deng, J.J.; Leung, C.H.C. Towards Learning a Joint Representation from Transformer in Multimodal Emotion Recognition. In Brain Informatics; Mahmud, M., Kaiser, M.S., Vassanelli, S., Dai, Q., Zhong, N., Eds.; Springer: Cham, Switzerland, 2021;
pp. 179â€“188.
</br>
[5] Georage, Barnum. et al. "On The Benefits of Early Fusion in Multimodal Representation Learning." arXiv preprint arXiv:2011.07171 (2020).
</br>
[6] K. Gadzicki, R. Khamsehashari and C. Zetzsche, "Early vs Late Fusion in Multimodal Convolutional Neural Networks," 2020 IEEE 23rd International Conference on Information Fusion (FUSION), Rustenburg, South Africa, 2020, pp. 1-6.
</br>
[7] Liang, Chen, et al. "S+ page: A speaker and position-aware graph neural network model for emotion recognition in conversation." arXiv preprint arXiv:2112.12389 (2021).
</br>
[8] Poria, Soujanya, et al. "Meld: A multimodal multi-party dataset for emotion recognition in conversations." arXiv preprint arXiv:1810.02508 (2018).
</br>
[9] K. J. Noh and H. Jeong, â€œKEMDy20,â€ https://nanum.etri.re.kr/share/kjnoh/KEMDy20?lang=ko_KR 
